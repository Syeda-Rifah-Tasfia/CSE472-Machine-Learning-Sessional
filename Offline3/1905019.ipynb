{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as ds\n",
    "from sklearn import datasets\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, inputDimensions, outputDimensions):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.weights = np.random.randn(outputDimensions, inputDimensions)\n",
    "        self.biases = np.zeros((outputDimensions, 1))\n",
    "\n",
    "    def forwardPass(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.weights, input) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backwardPass(self, dL_dout, alpha):\n",
    "        dw = np.dot(dL_dout, self.input.T)\n",
    "        db = np.sum(dL_dout, axis=1, keepdims=True)\n",
    "\n",
    "        di = np.dot(self.weights.T, dL_dout)\n",
    "\n",
    "        self.weights -= alpha * dw\n",
    "        self.biases -= alpha * db\n",
    "\n",
    "        return di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RELU:\n",
    "    ## matrix that passes through relu is (n[l], m)\n",
    "    def forwardPass(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backwardPass(self, x):\n",
    "        return x * (self.input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def calcSoftmax(self, Y):\n",
    "        #calculate from an 1D array\n",
    "        exps = np.exp(Y - np.max(Y, axis=0, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "    \n",
    "    def forwardPass(self, Y):\n",
    "        return self.calcSoftmax(Y)\n",
    "    \n",
    "    def backwardPass(self, Y):\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat + 1e-9)) / y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loaders:\n",
    "    def train_data(self, epochs, learnRate, input_dimensions, output_dimensions, train_loader, dense1, activation1, dense2):\n",
    "        #initialize layers\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(images.size(0), -1).numpy()\n",
    "                labels_onehot = np.eye(output_dimensions)[labels.numpy()]\n",
    "\n",
    "                #forward pass\n",
    "                dense1_output = dense1.forwardPass(images.T)\n",
    "                activation1_output = activation1.forwardPass(dense1_output)\n",
    "                dense2_output = dense2.forwardPass(activation1_output)\n",
    "                y_pred = Softmax().calcSoftmax(dense2_output)\n",
    "\n",
    "\n",
    "                loss = crossEntropyLoss(labels_onehot, y_pred.T)\n",
    "                total_loss += loss\n",
    "\n",
    "                #backward pass\n",
    "                loss_gradient = y_pred.T - labels_onehot\n",
    "                dh = dense2.backwardPass(loss_gradient.T, learnRate)\n",
    "                do1 = activation1.backwardPass(dh)\n",
    "                dense1.backwardPass(do1, learnRate)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    def test_data(self, test_loader, dense1, activation1, dense2):\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.size(0), -1).numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            #write like train\n",
    "            \n",
    "            dense1_output = dense1.forwardPass(images.T)\n",
    "            activation1_output = activation1.forwardPass(dense1_output)\n",
    "            dense2_output = dense2.forwardPass(activation1_output)\n",
    "            y_pred = Softmax().calcSoftmax(dense2_output)\n",
    "\n",
    "            predictions = np.argmax(y_pred, axis=0)\n",
    "            correct += np.sum(predictions == labels)\n",
    "            total += labels.shape[0]\n",
    "\n",
    "\n",
    "        print(f\"Accuracy: {correct/total}\")\n",
    "\n",
    "    def savetoPickle(self, dense1, dense2, filename):\n",
    "        weghts_and_biases = {\n",
    "            \"dense1_weights\": dense1.weights,\n",
    "            \"dense1_biases\": dense1.biases,\n",
    "            \"dense2_weights\": dense2.weights,\n",
    "            \"dense2_biases\": dense2.biases\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(weghts_and_biases, file)\n",
    "\n",
    "        print(\"Saved to pickle file\")\n",
    "\n",
    "    def loadfromPickle(self, filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            weights_and_biases = pickle.load(file)\n",
    "\n",
    "        dense1 = DenseLayer(28*28, 128)\n",
    "        dense1.weights = weights_and_biases[\"dense1_weights\"]\n",
    "        dense1.biases = weights_and_biases[\"dense1_biases\"]\n",
    "\n",
    "        dense2 = DenseLayer(128, 10)\n",
    "        dense2.weights = weights_and_biases[\"dense2_weights\"]\n",
    "        dense2.biases = weights_and_biases[\"dense2_biases\"]\n",
    "\n",
    "        return dense1, dense2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./FMNIST', train=True, download=True, transform=transform)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='./FMNIST', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Loaders()\n",
    "\n",
    "input_dimensions = 28*28\n",
    "output_dimensions = 10\n",
    "numOfNeurons = 20\n",
    "\n",
    "epochs = 5\n",
    "learnRate = 0.1\n",
    "\n",
    "dense1 = DenseLayer(input_dimensions, numOfNeurons)\n",
    "activation1 = RELU()\n",
    "dense2 = DenseLayer(numOfNeurons, output_dimensions)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 24.50027656219117\n",
      "Epoch 2, Loss: 23.509830998466914\n",
      "Epoch 3, Loss: 23.503460824627844\n",
      "Epoch 4, Loss: 23.528032840634907\n",
      "Epoch 5, Loss: 23.532574532262622\n"
     ]
    }
   ],
   "source": [
    "loaders.train_data(epochs, learnRate, input_dimensions, output_dimensions, train_loader, dense1, activation1, dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "loaders.test_data(test_loader, dense1, activation1, dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"1905019.pkl\"\n",
    "loaders.savetoPickle(dense1, dense2, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders.loadfromPickle(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
